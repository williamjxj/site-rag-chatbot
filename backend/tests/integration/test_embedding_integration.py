"""Integration tests for embedding generation and retrieval."""

import os
from unittest.mock import patch

import pytest

# Check if required dependencies are available
try:
    import pgvector  # noqa: F401
    PGVECTOR_AVAILABLE = True
except ImportError:
    PGVECTOR_AVAILABLE = False

try:
    import sentence_transformers  # noqa: F401
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

try:
    from src.rag.embedder import embed_texts
    EMBEDDER_AVAILABLE = True
except ImportError as e:
    EMBEDDER_AVAILABLE = False
    EMBEDDER_IMPORT_ERROR = e

# Embedder is only available if sentence-transformers is installed
EMBEDDER_AVAILABLE = EMBEDDER_AVAILABLE and SENTENCE_TRANSFORMERS_AVAILABLE

# Only import retriever if pgvector is available
if PGVECTOR_AVAILABLE:
    try:
        from src.rag.retriever import retrieve
        RETRIEVER_AVAILABLE = True
    except ImportError:
        RETRIEVER_AVAILABLE = False
else:
    RETRIEVER_AVAILABLE = False


@pytest.mark.integration
@pytest.mark.skipif(not EMBEDDER_AVAILABLE, reason="Embedder dependencies not available")
@patch.dict(os.environ, {"EMBEDDING_PROVIDER": "local"}, clear=False)
def test_end_to_end_ingestion_with_free_model():
    """Test that ingestion can generate embeddings using free model without API keys."""
    # This test verifies the core functionality: embedding generation works without API keys
    texts = [
        "This is a test document about artificial intelligence.",
        "Machine learning is a subset of AI.",
        "Natural language processing enables computers to understand text.",
    ]
    
    # Should work without API keys when provider is set to local
    embeddings = embed_texts(texts)
    
    assert len(embeddings) == 3
    assert len(embeddings[0]) == 384  # all-MiniLM-L6-v2 produces 384 dimensions
    assert len(embeddings[1]) == 384
    assert len(embeddings[2]) == 384
    
    # Verify embeddings are not all zeros
    assert any(abs(val) > 0.001 for val in embeddings[0])


@pytest.mark.integration
@pytest.mark.skipif(not RETRIEVER_AVAILABLE, reason="Retriever dependencies (pgvector) not available")
@patch.dict(os.environ, {"EMBEDDING_PROVIDER": "local"}, clear=False)
def test_retrieval_with_free_model_embeddings():
    """Test that retrieval works with embeddings generated by free model."""
    # Generate query embedding
    query = "What is artificial intelligence?"
    query_embedding = embed_texts([query])[0]
    
    assert len(query_embedding) == 384
    
    # Retrieve similar chunks (this requires database to be set up)
    # Note: This test may require database fixtures or mocking
    # For now, we verify the embedding dimension is correct
    assert isinstance(query_embedding, list)
    assert all(isinstance(val, (int, float)) for val in query_embedding)


@pytest.mark.integration
@pytest.mark.skipif(not EMBEDDER_AVAILABLE, reason="Embedder dependencies not available")
@patch.dict(os.environ, {"EMBEDDING_PROVIDER": "local", "EMBEDDING_API_KEY": "", "OPENAI_API_KEY": ""}, clear=False)
def test_no_api_key_errors_with_free_model():
    """Test that system operates without API key validation errors when using free model."""
    # Should not raise errors about missing API keys
    texts = ["Test document"]
    
    # This should work without any API keys
    embeddings = embed_texts(texts)
    
    assert len(embeddings) == 1
    assert len(embeddings[0]) == 384


@pytest.mark.integration
@pytest.mark.skipif(not EMBEDDER_AVAILABLE, reason="Embedder dependencies not available")
@patch.dict(os.environ, {"EMBEDDING_PROVIDER": "local"}, clear=False)
def test_batch_embedding_generation():
    """Test that multiple texts can be embedded in a single batch."""
    texts = [f"Document {i} about topic {i % 3}" for i in range(10)]
    
    embeddings = embed_texts(texts)
    
    assert len(embeddings) == 10
    assert all(len(emb) == 384 for emb in embeddings)
    
    # Verify embeddings are different (not all identical)
    first_emb = embeddings[0]
    different_count = sum(1 for emb in embeddings[1:] if emb != first_emb)
    assert different_count > 0  # At least some embeddings should differ


@pytest.mark.integration
@pytest.mark.skipif(not EMBEDDER_AVAILABLE, reason="Embedder dependencies not available")
@patch.dict(os.environ, {"EMBEDDING_PROVIDER": "local"}, clear=False)
def test_empty_text_handling():
    """Test that empty or very short texts are handled correctly."""
    texts = ["", "a", "This is a normal length text."]
    
    embeddings = embed_texts(texts)
    
    assert len(embeddings) == 3
    assert all(len(emb) == 384 for emb in embeddings)


@pytest.mark.integration
@pytest.mark.performance
@pytest.mark.skipif(not EMBEDDER_AVAILABLE, reason="Embedder dependencies not available")
@patch.dict(os.environ, {"EMBEDDING_PROVIDER": "local"}, clear=False)
def test_embedding_generation_performance_20_page_doc():
    """Test that embedding generation completes for a 20-page document within 60 seconds."""
    import time
    
    # Simulate a 20-page document: ~1800 chars per page, 20 pages = ~36k chars
    # Split into typical chunks of ~1800 chars each = ~20 chunks
    texts = [
        f"This is chunk {i} of a simulated 20-page document. " * 50  # ~1800 chars per chunk
        for i in range(20)
    ]
    
    start_time = time.time()
    embeddings = embed_texts(texts)
    elapsed_time = time.time() - start_time
    
    assert len(embeddings) == 20
    assert all(len(emb) == 384 for emb in embeddings)
    
    # Performance target: <60 seconds for 20-page document
    assert elapsed_time < 60.0, f"Embedding generation took {elapsed_time:.2f}s, target is <60s"
    
    # Log performance for monitoring
    print(f"\nPerformance: Generated 20 embeddings in {elapsed_time:.2f}s ({20/elapsed_time:.1f} chunks/sec)")


@pytest.mark.integration
@pytest.mark.performance
@pytest.mark.skipif(not EMBEDDER_AVAILABLE, reason="Embedder dependencies not available")
@patch.dict(os.environ, {"EMBEDDING_PROVIDER": "local"}, clear=False)
def test_batch_processing_performance_100_chunks():
    """Test that batch processing handles 100 chunks in under 10 minutes."""
    import time
    
    # Generate 100 text chunks (typical document set: 10-50 documents, ~2-3 chunks each)
    texts = [
        f"This is chunk {i} from document {i // 3}. " * 30  # ~900 chars per chunk
        for i in range(100)
    ]
    
    start_time = time.time()
    embeddings = embed_texts(texts)
    elapsed_time = time.time() - start_time
    
    assert len(embeddings) == 100
    assert all(len(emb) == 384 for emb in embeddings)
    
    # Performance target: <10 minutes (600 seconds) for 100 chunks
    assert elapsed_time < 600.0, f"Batch processing took {elapsed_time:.2f}s, target is <600s"
    
    # Log performance for monitoring
    chunks_per_sec = 100 / elapsed_time if elapsed_time > 0 else 0
    print(f"\nPerformance: Processed 100 chunks in {elapsed_time:.2f}s ({chunks_per_sec:.1f} chunks/sec)")


# Note: Tests for mixed dimensions (User Story 2) would require database setup
# with existing 1536-dim embeddings. For POC, re-ingestion is recommended
# to ensure consistent dimensions. The retrieval logic handles dimension
# mismatches gracefully by only returning chunks with matching dimensions.

